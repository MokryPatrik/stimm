# ./docker-compose.yml (version simplifiée)

services:
  llm-server:
    # Construire l'image à partir du Dockerfile dans le répertoire courant
    build: .
    image: voicebot-llm-server
    container_name: llm-server
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - LLM_HF_MODEL=${LLM_HF_MODEL}
      - LLM_CONVERT_SCRIPT=${LLM_CONVERT_SCRIPT}
      - LLM_CONVERT_ARGS=${LLM_CONVERT_ARGS}
      - LLM_CONVERT_INPUT_DIR=${LLM_CONVERT_INPUT_DIR}
      - LLM_MAX_OUTPUT_TOKENS=${LLM_MAX_OUTPUT_TOKENS}
      - LLM_MAX_BATCH_SIZE=${LLM_MAX_BATCH_SIZE}
      - LLM_MAX_INPUT_LEN=${LLM_MAX_INPUT_LEN}
      - LLM_MAX_SEQ_LEN=${LLM_MAX_SEQ_LEN}
      - LLM_MAX_NUM_TOKENS=${LLM_MAX_NUM_TOKENS}
      - LLM_BUILD_ARGS=${LLM_BUILD_ARGS}
      - LLM_QUANTIZE_SCRIPT=${LLM_QUANTIZE_SCRIPT}
      - LLM_QUANTIZE_OUTPUT_DIR=${LLM_QUANTIZE_OUTPUT_DIR}
      - LLM_QUANTIZE_EXTRA_ARGS=${LLM_QUANTIZE_EXTRA_ARGS}
    ports:
      - "8000:8000"
    # Volume pour persister le modèle et le moteur compilé
    volumes:
      - ../../models:/models
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    networks:
      voicebot-network:
        aliases:
          - llm

# Déclaration du volume nommé
volumes:
  llm-models:

networks:
  voicebot-network:
    name: voicebot-network
    driver: bridge
